{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crystal Ball\n",
    "AI providing buy/sell/hold actions based on candlestick data.\n",
    "\n",
    "## Binance\n",
    "https://www.kaggle.com/code/lucasmorin/getting-all-1m-data-from-binance/notebook\n",
    "\n",
    "https://developers.binance.com/docs/binance-spot-api-docs/rest-api/market-data-endpoints#klinecandlestick-data\n",
    "\n",
    "## Reinforcement Learning\n",
    "### Pong from Pixels\n",
    "https://karpathy.github.io/2016/05/31/rl/\n",
    "\n",
    "### Stable Baselines\n",
    "https://stable-baselines3.readthedocs.io/en/master/\n",
    "https://anaconda.org/conda-forge/stable-baselines3\n",
    "\n",
    "### Policy Gradient Methods\n",
    "https://youtu.be/5P7I-xPq8u8?si=hXVvvLb1S8XcWGfz\n",
    "\n",
    "### Example\n",
    "https://towardsdatascience.com/how-to-train-an-ai-to-play-any-game-f1489f3bc5c\n",
    "https://github.com/guszejnovdavid/custom_game_reinforcement_learning/blob/main/custom_game_reinforcement_learning.ipynb\n",
    "\n",
    "\n",
    "Installation\n",
    "~~~shell\n",
    "$ conda install pytorch torchvision torchaudio pytorch-cuda pandas -c pytorch -c nvidia\n",
    "$ conda install pyarrow -c conda-forge\n",
    "$ conda install conda-forge::stable-baselines3\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# CUDA device setup\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "from datetime import date, datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "API_BASE = 'https://api.binance.com/api/v3/'\n",
    "\n",
    "LABELS = [\n",
    "    'open_time',\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'volume',\n",
    "    'close_time',\n",
    "    'quote_asset_volume',\n",
    "    'number_of_trades',\n",
    "    'taker_buy_base_asset_volume',\n",
    "    'taker_buy_quote_asset_volume',\n",
    "    'ignore'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(symbol, interval='1m', start_time=0, limit=1000):\n",
    "    \"\"\"Use a GET request to retrieve a batch of candlesticks. Process the JSON into a pandas\n",
    "    dataframe and return it. If not successful, return an empty dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'startTime': start_time,\n",
    "        'limit': limit\n",
    "    }\n",
    "    try:\n",
    "        # timeout should also be given as a parameter to the function\n",
    "        response = requests.get(f'{API_BASE}klines', params, timeout=30)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print('Connection error, Cooling down for 5 mins...')\n",
    "        time.sleep(5 * 60)\n",
    "        return get_batch(symbol, interval, start_time, limit)\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print('Timeout, Cooling down for 5 min...')\n",
    "        time.sleep(5 * 60)\n",
    "        return get_batch(symbol, interval, start_time, limit)\n",
    "    \n",
    "    except requests.exceptions.ConnectionResetError:\n",
    "        print('Connection reset by peer, Cooling down for 5 min...')\n",
    "        time.sleep(5 * 60)\n",
    "        return get_batch(symbol, interval, start_time, limit)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return pd.DataFrame(response.json(), columns=LABELS)\n",
    "    print(f'Got erroneous response back: {response}')\n",
    "    return pd.DataFrame([])\n",
    "\n",
    "# TODO: No new data is available on this channel?\n",
    "def all_candles_to_csv(base, quote, interval='1m'):\n",
    "    \"\"\"Collect a list of candlestick batches with all candlesticks of a trading pair,\n",
    "    concat into a dataframe and write it to CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # see if there is any data saved on disk already\n",
    "    try:\n",
    "        batches = [pd.read_csv(f'data/{base}-{quote}.csv')]\n",
    "        last_timestamp = batches[-1]['open_time'].max()\n",
    "    except FileNotFoundError:\n",
    "        batches = [pd.DataFrame([], columns=LABELS)]\n",
    "        last_timestamp = 0\n",
    "    old_lines = len(batches[-1].index)\n",
    "\n",
    "    # gather all candlesticks available, starting from the last timestamp loaded from disk or 0\n",
    "    # stop if the timestamp that comes back from the api is the same as the last one\n",
    "    previous_timestamp = None\n",
    "\n",
    "    while previous_timestamp != last_timestamp:\n",
    "        # stop if we reached data from today\n",
    "        if date.fromtimestamp(last_timestamp / 1000) >= date.today():\n",
    "            break\n",
    "\n",
    "        previous_timestamp = last_timestamp\n",
    "\n",
    "        new_batch = get_batch(\n",
    "            symbol=base+quote,\n",
    "            interval=interval,\n",
    "            start_time=last_timestamp+1\n",
    "        )\n",
    "\n",
    "        # requesting candles from the future returns empty\n",
    "        # also stop in case response code was not 200\n",
    "        if new_batch.empty:\n",
    "            break\n",
    "\n",
    "        last_timestamp = new_batch['open_time'].max()\n",
    "\n",
    "        # sometimes no new trades took place yet on date.today();\n",
    "        # in this case the batch is nothing new\n",
    "        if previous_timestamp == last_timestamp:\n",
    "            break\n",
    "\n",
    "        batches.append(new_batch)\n",
    "        last_datetime = datetime.fromtimestamp(last_timestamp / 1000)\n",
    "\n",
    "        covering_spaces = 20 * ' '\n",
    "        print(datetime.now(), base, quote, interval, str(last_datetime)+covering_spaces, end='\\r', flush=True)\n",
    "\n",
    "    # write clean version of csv to parquet\n",
    "    parquet_name = f'{base}-{quote}.parquet'\n",
    "    full_path = f'compressed/{parquet_name}'\n",
    "    df = pd.concat(batches, ignore_index=True)\n",
    "    df = quick_clean(df)\n",
    "    write_raw_to_parquet(df, full_path)\n",
    "\n",
    "    # in the case that new data was gathered write it to disk\n",
    "    if len(batches) > 1:\n",
    "        df.to_csv(f'data/{base}-{quote}.csv', index=False)\n",
    "        return len(df.index) - old_lines\n",
    "    return 0\n",
    "\n",
    "def set_dtypes(df):\n",
    "    \"\"\"\n",
    "    set datetimeindex and convert all columns in pd.df to their proper dtype\n",
    "    assumes csv is read raw without modifications; pd.read_csv(csv_filename)\"\"\"\n",
    "\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df = df.set_index('open_time', drop=True)\n",
    "\n",
    "    df = df.astype(dtype={\n",
    "        'open': 'float64',\n",
    "        'high': 'float64',\n",
    "        'low': 'float64',\n",
    "        'close': 'float64',\n",
    "        'volume': 'float64',\n",
    "        'close_time': 'datetime64[ms]',\n",
    "        'quote_asset_volume': 'float64',\n",
    "        'number_of_trades': 'int64',\n",
    "        'taker_buy_base_asset_volume': 'float64',\n",
    "        'taker_buy_quote_asset_volume': 'float64',\n",
    "        'ignore': 'float64'\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_dtypes_compressed(df):\n",
    "    \"\"\"Create a `DatetimeIndex` and convert all critical columns in pd.df to a dtype with low\n",
    "    memory profile. Assumes csv is read raw without modifications; `pd.read_csv(csv_filename)`.\"\"\"\n",
    "\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')\n",
    "    df = df.set_index('open_time', drop=True)\n",
    "\n",
    "    df = df.astype(dtype={\n",
    "        'open': 'float32',\n",
    "        'high': 'float32',\n",
    "        'low': 'float32',\n",
    "        'close': 'float32',\n",
    "        'volume': 'float32',\n",
    "        'number_of_trades': 'uint16',\n",
    "        'quote_asset_volume': 'float32',\n",
    "        'taker_buy_base_asset_volume': 'float32',\n",
    "        'taker_buy_quote_asset_volume': 'float32'\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def assert_integrity(df):\n",
    "    \"\"\"make sure no rows have empty cells or duplicate timestamps exist\"\"\"\n",
    "\n",
    "    assert df.isna().all(axis=1).any() == False\n",
    "    assert df['open_time'].duplicated().any() == False\n",
    "\n",
    "\n",
    "def quick_clean(df):\n",
    "    \"\"\"clean a raw dataframe\"\"\"\n",
    "\n",
    "    # drop dupes\n",
    "    dupes = df['open_time'].duplicated().sum()\n",
    "    if dupes > 0:\n",
    "        df = df[df['open_time'].duplicated() == False]\n",
    "\n",
    "    # sort by timestamp, oldest first\n",
    "    df.sort_values(by=['open_time'], ascending=False)\n",
    "\n",
    "    # just a doublcheck\n",
    "    assert_integrity(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_raw_to_parquet(df, full_path):\n",
    "    \"\"\"takes raw df and writes a parquet to disk\"\"\"\n",
    "\n",
    "    # some candlesticks do not span a full minute\n",
    "    # these points are not reliable and thus filtered\n",
    "    df = df[~(df['open_time'] - df['close_time'] != -59999)]\n",
    "\n",
    "    # `close_time` column has become redundant now, as is the column `ignore`\n",
    "    df = df.drop(['close_time', 'ignore'], axis=1)\n",
    "\n",
    "    df = set_dtypes_compressed(df)\n",
    "\n",
    "    # give all pairs the same nice cut-off\n",
    "    df = df[df.index < str(date.today())]\n",
    "\n",
    "    df.to_parquet(full_path)\n",
    "\n",
    "\n",
    "def groom_data(dirname='data'):\n",
    "    \"\"\"go through data folder and perform a quick clean on all csv files\"\"\"\n",
    "\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith('.csv'):\n",
    "            full_path = f'{dirname}/{filename}'\n",
    "            quick_clean(pd.read_csv(full_path)).to_csv(full_path)\n",
    "\n",
    "\n",
    "def compress_data(dirname='data'):\n",
    "    \"\"\"go through data folder and rewrite csv files to parquets\"\"\"\n",
    "\n",
    "    os.makedirs('compressed', exist_ok=True)\n",
    "    for filename in os.listdir(dirname):\n",
    "        if filename.endswith('.csv'):\n",
    "            full_path = f'{dirname}/{filename}'\n",
    "\n",
    "            df = pd.read_csv(full_path)\n",
    "\n",
    "            new_filename = filename.replace('.csv', '.parquet')\n",
    "            new_full_path = f'compressed/{new_filename}'\n",
    "            write_raw_to_parquet(df, new_full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = pd.DataFrame(requests.get(f'{API_BASE}exchangeInfo').json()['symbols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ticker = {\n",
    "    'Bitcoin Cash':'BCH',\n",
    "    'Binance Coin':'BNB',\n",
    "    'Bitcoin':'BTC',\n",
    "    'EOS.IO':'EOS',\n",
    "    'Ethereum Classic':'ETC',\n",
    "    'Ethereum':'ETH',\n",
    "    'Litecoin':'LTC',\n",
    "    'Monero':'XMR',\n",
    "    'TRON':'TRX',\n",
    "    'Stellar':'XLM',\n",
    "    'Cardano':'ADA',\n",
    "    'IOTA':'IOTA',\n",
    "    'Maker':'MKR',\n",
    "    'Dogecoin':'DOGE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USDT', 'USDC', 'TUSD', 'BUSD', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'USDS', 'BUSD', 'USDP', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'USDS', 'BUSD', 'USDP', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD', 'FDUSD']\n",
      "['USDT', 'USDC', 'TUSD', 'BUSD', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD', 'USDP', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD', 'FDUSD']\n",
      "['USDT', 'BUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD', 'FDUSD']\n",
      "['USDT', 'TUSD', 'USDC', 'BUSD', 'FDUSD']\n",
      "['USDT', 'BUSD', 'FDUSD']\n",
      "['USDT', 'BUSD']\n",
      "['USDT', 'USDC', 'BUSD', 'TUSD', 'FDUSD']\n"
     ]
    }
   ],
   "source": [
    "for a in dict_ticker:\n",
    "    quoteAssetsa = all_symbols[all_symbols.baseAsset == dict_ticker[a]].quoteAsset.unique()\n",
    "    USDquoteAssetsa = [qA for qA in quoteAssetsa if 'USD' in qA]\n",
    "    print(USDquoteAssetsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'BUSD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = [(dict_ticker[a],quote) for a in dict_ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BCH', 'BUSD'),\n",
       " ('BNB', 'BUSD'),\n",
       " ('BTC', 'BUSD'),\n",
       " ('EOS', 'BUSD'),\n",
       " ('ETC', 'BUSD'),\n",
       " ('ETH', 'BUSD'),\n",
       " ('LTC', 'BUSD'),\n",
       " ('XMR', 'BUSD'),\n",
       " ('TRX', 'BUSD'),\n",
       " ('XLM', 'BUSD'),\n",
       " ('ADA', 'BUSD'),\n",
       " ('IOTA', 'BUSD'),\n",
       " ('MKR', 'BUSD'),\n",
       " ('DOGE', 'BUSD')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-30 18:41:19.028663 1/14 Already up to date with BCH-BUSD\n",
      "2025-01-30 18:41:21.847687 2/14 Already up to date with BNB-BUSD\n",
      "2025-01-30 18:41:24.690953 3/14 Already up to date with BTC-BUSD\n",
      "2025-01-30 18:41:27.250552 4/14 Already up to date with EOS-BUSD\n",
      "2025-01-30 18:41:29.889487 5/14 Already up to date with ETC-BUSD\n",
      "2025-01-30 18:41:32.782606 6/14 Already up to date with ETH-BUSD\n",
      "2025-01-30 18:41:35.477090 7/14 Already up to date with LTC-BUSD\n",
      "2025-01-30 18:41:37.897723 8/14 Already up to date with XMR-BUSD\n",
      "2025-01-30 18:41:40.528792 9/14 Already up to date with TRX-BUSD\n",
      "2025-01-30 18:41:43.071846 10/14 Already up to date with XLM-BUSD\n",
      "2025-01-30 18:41:45.749013 11/14 Already up to date with ADA-BUSD\n",
      "2025-01-30 18:41:47.838450 12/14 Already up to date with IOTA-BUSD\n",
      "2025-01-30 18:41:50.058091 13/14 Already up to date with MKR-BUSD\n",
      "2025-01-30 18:41:52.328712 14/14 Already up to date with DOGE-BUSD\n"
     ]
    }
   ],
   "source": [
    "# make sure data folders exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('compressed', exist_ok=True)\n",
    "\n",
    "# do a full update on all pairs\n",
    "n_count = len(all_pairs)\n",
    "for n, pair in enumerate(all_pairs, 1):\n",
    "    base, quote = pair\n",
    "    new_lines = all_candles_to_csv(base=base, quote=quote)\n",
    "    if new_lines > 0:\n",
    "        print(f'{datetime.now()} {n}/{n_count} Wrote {new_lines} new lines to file for {base}-{quote}')\n",
    "    else:\n",
    "        print(f'{datetime.now()} {n}/{n_count} Already up to date with {base}-{quote}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Checking out how to handle parquet data ...\n",
    "\n",
    "#import pyarrow as pa\n",
    "#import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "#import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "#from torch.utils.data import get_worker_info\n",
    "from torch.multiprocessing import Queue\n",
    "\n",
    "class IterableParquetDataset(IterableDataset):\n",
    "    def __init__(self, path, process_func=None):\n",
    "        super().__init__()\n",
    "        self.__process_func = process_func\n",
    "\n",
    "        dataset = ds.dataset(path)\n",
    "        self.__batches = Queue()\n",
    "        [self.__batches.put(batch) for batch in dataset.to_batches()]\n",
    "\n",
    "    def __iter__(self):\n",
    "        while not self.__batches.empty():\n",
    "            batch = self.__batches.get().to_pydict()\n",
    "            if self.__process_func is not None:\n",
    "                batch.update(self.__process_func(batch))\n",
    "            yield batch\n",
    "        self.__batches.close()\n",
    "\n",
    "from typing import List\n",
    "\n",
    "class BinanceDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self._dict = ds.dataset(path).to_pydict()\n",
    "\n",
    "    def __len__(self):\n",
    "         return len(self._dict)\n",
    "\n",
    "    def __getitem__(self, index: List):\n",
    "        return { key: self._dict[key][index] for key in self._dict.keys() }\n",
    "        #x, y = torch.load(self.files[index])\n",
    "        #return x, y\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import pyarrow.dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data: pyarrow.dataset.Dataset, label_column: str) -> None:\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return self.data.count_rows()\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[list[Any], Any]:\n",
    "        row = self.data.take([index]).to_pylist()[0]\n",
    "        x = [v for k, v in row.items() if k != self.label_column]\n",
    "        y = row[self.label_column]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "data = pyarrow.dataset.dataset('dataset_dir', format='parquet')\n",
    "torch_dataset = CustomDataset(data=data, label_column='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'pyarrow._dataset.FileSystemDataset' object has no attribute 'to_pydict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m iterable \u001b[38;5;28;01mif\u001b[39;00m condition(x))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# digging into the data\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#dataset = IterableParquetDataset('compressed/BTC-BUSD.parquet')\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#for batch in dataset:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#    print(len(batch['open']))\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m dataset2 \u001b[38;5;241m=\u001b[39m \u001b[43mBinanceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcompressed/BTC-BUSD.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset2[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m, in \u001b[0;36mBinanceDataset.__init__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'pyarrow._dataset.FileSystemDataset' object has no attribute 'to_pydict'"
     ]
    }
   ],
   "source": [
    "# TODO: Just testing ...\n",
    "\n",
    "def first(iterable, condition = lambda x: True):\n",
    "    \"\"\"\n",
    "    Returns the first item in the `iterable` that\n",
    "    satisfies the `condition`.\n",
    "\n",
    "    If the condition is not given, returns the first item of\n",
    "    the iterable.\n",
    "\n",
    "    Raises `StopIteration` if no item satysfing the condition is found.\n",
    "\n",
    "    >>> first( (1,2,3), condition=lambda x: x % 2 == 0)\n",
    "    2\n",
    "    >>> first(range(3, 100))\n",
    "    3\n",
    "    >>> first( () )\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    StopIteration\n",
    "    \"\"\"\n",
    "\n",
    "    return next(x for x in iterable if condition(x))\n",
    "\n",
    "# digging into the data\n",
    "#dataset = IterableParquetDataset('compressed/BTC-BUSD.parquet')\n",
    "\n",
    "# Print the keys in the dictionary\n",
    "#print(first(dataset).keys())\n",
    "\n",
    "#for batch in dataset:\n",
    "#    print(len(batch['open']))\n",
    "\n",
    "dataset2 = BinanceDataset('compressed/BTC-BUSD.parquet')\n",
    "print(dataset2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
